{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import operator\n",
    "import re, string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Philippine Standard Geographic Code Reference File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psgc = pd.read_csv(\"psgc/data/processed/clean-psgc.csv.gz\",\n",
    "                   dtype={'code':'object'},\n",
    "                   compression=\"gzip\",\n",
    "                   encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psgc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psgc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# totally drop places that are just \"capital\" or \"not a province\". I think in the previous cleanups of the PSGC file anything\n",
    "# in parentheses was turned into a new row. As such, \"Capital\" was often turned into a new row.\n",
    "\n",
    "psgc = psgc[psgc.location.isin([\"CAPITAL\",\"NOT A PROVINCE\"]) == False].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interlevel lower\n",
    "psgc.interlevel = psgc.interlevel.str.lower()\n",
    "psgc.interlevel = psgc.interlevel.replace({\"mun\": \"municity\", \"city\": \"municity\"}) #combine municity for now\n",
    "\n",
    "# Clean location column a bit. but not too much because we'll use this as the \"canonical\" name\n",
    "psgc['location'] = psgc.location.str.replace(r\"NOT A PROVINCE|CAPITAL|\\(|\\)\", \"\").str.strip()\n",
    "psgc = psgc.drop_duplicates(subset=[\"code\", \"location\", \"interlevel\"], keep=\"first\")\n",
    "psgc = psgc.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psgc.interlevel.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename districts as simply \"Metro Manila\", \"Metropolitan Manila\", \"National Capital Region\" or \"NCR\"\n",
    "\n",
    "ncr = psgc[(psgc.code == \"130000000\")]\n",
    "ncr.loc[:,\"location\"] = ncr.location.str.replace(\"NATIONAL  REGION\", \"NATIONAL CAPITAL REGION\")\n",
    "ncr.loc[:,\"location\"] = ncr.location.str.replace(\"NATIONAL  REGION\", \"NATIONAL CAPITAL REGION\")\n",
    "ncr = ncr.append(pd.Series({\"code\":\"130000000\", \"location\": \"METRO MANILA\",\"interlevel\":\"reg\",\"original\":False}), ignore_index=True)\n",
    "ncr = ncr.append(pd.Series({\"code\":\"130000000\", \"location\": \"METROPOLITAN MANILA\", \"interlevel\": \"reg\", \"original\": False}), ignore_index=True)\n",
    "ncr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove districts and replace NCR region rows from reference file for now with cleaned up NCR rows\n",
    "\n",
    "psgc = psgc[psgc.interlevel != 'dist'].reset_index(drop=True) # exclude districts\n",
    "psgc = psgc[psgc.code != '130000000'].reset_index(drop=True) # exclude original ncr region rows\n",
    "print(len(psgc))\n",
    "psgc = psgc.append(ncr, ignore_index=True) # append cleaned up ncr region rows\n",
    "print(len(psgc))\n",
    "psgc.head()\n",
    "psgc[psgc.code == \"130000000\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add as aliases abbreviations of north, south, east, west. for example, northern samar will have an alias n. samar.\n",
    "\n",
    "nsew = re.compile(r\"^NORTH(ERN)? |^SOUTH(ERN)? |^EAST(ERN?)? |^WEST(ERN)? \")\n",
    "nsew_abbrev = psgc[psgc.location.str.contains(nsew)].location.str.split().str.get(0).str.slice(0,1)\n",
    "nsew_abbrev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsew_locs = psgc[psgc.location.str.contains(nsew)]\n",
    "nsew_locs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsew_locs.loc[:, 'location'] = nsew_abbrev.str.cat(psgc[psgc.location.str.contains(nsew)].location.str.replace(\"^NORTH(ERN)? |^SOUTH(ERN)? |^EAST(ERN?)? |^WEST(ERN)? \",\"\").str.strip(),sep=\" \")\n",
    "nsew_locs.loc[:, 'original'] = False\n",
    "nsew_locs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psgc = pd.concat([psgc, nsew_locs], ignore_index=True)\n",
    "psgc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill interlevels for isabela, cotabato\n",
    "\n",
    "psgc.loc[psgc.interlevel.isnull(), \"interlevel\"] = \"municity\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need is reference file that contains the higher-level administrative territories in separate columns. This is so we can create a single \"master string\" that we will use for matching. We'll try this instead of matching each component individually.\n",
    "\n",
    "First, create a dictionary of the rankings of various administrative levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm_rank= {'reg': 1, 'prov': 2, 'dist': 2, 'city': 3, 'mun': 3, 'municity': 3, 'submun': 3, 'bgy': 4}\n",
    "adm_rank_list = sorted(adm_rank, key=lambda k: adm_rank[k])\n",
    "psgc['adm_rank'] = psgc.interlevel.map(adm_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psgc.interlevel.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll apply this later as a separate column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that will add to our dataframe columns with the PSGC codes\n",
    "of each location's higher level administrative territories. We'll then use this to fill the name columns \n",
    "with their corresponding place names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_higher_level_codes(df):\n",
    "        \n",
    "    # Below is a dictionary of administrative hierarchy levels ranks and the stop string positions inside the PSG code.\n",
    "    \n",
    "    adm_rank = {1: 2,\n",
    "                2: 4,\n",
    "                3: 6,\n",
    "                4: 9} \n",
    "    \n",
    "    # Loop through each administrative level. \n",
    "    # Create additional columns for each administrative level with the suffixes _code and _name.\n",
    "    # Fill each column with the names and codes of the higher level administrative territories in which\n",
    "    # a place is located.\n",
    "    \n",
    "    for adm_level in adm_rank.keys():\n",
    "        \n",
    "        # create code cols        \n",
    "        adm_code_col = \"adm{}_code\".format(adm_level)\n",
    "        \n",
    "        df[adm_code_col] = None\n",
    "        \n",
    "        # find the administrative levels that are higher than the current one\n",
    "        \n",
    "        higher_adm_levels = [l for l in adm_rank.keys() if l <= adm_level]\n",
    "                \n",
    "        for higher_level in higher_adm_levels:\n",
    "            \n",
    "            # higher adm level colum names\n",
    "            \n",
    "            higher_level_code_col = \"adm{}_code\".format(higher_level)\n",
    "            \n",
    "            # stop position of PSG code for this adm level\n",
    "            \n",
    "            stop_position = adm_rank[higher_level] \n",
    "            \n",
    "            # derive higher level admin codes for each row\n",
    "            \n",
    "            codes = df.loc[df.adm_rank >= higher_level, \"code\"].str.slice(start=0, stop=stop_position).str.pad(9, side=\"right\", fillchar=\"0\")\n",
    "            df.loc[df.adm_rank >= higher_level,higher_level_code_col] = codes\n",
    "            \n",
    "            # derive higher level admin names for each row\n",
    "            \n",
    "            higher_level_name_col = \"adm{}_name\".format(higher_level)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psgc_unpivot = fill_higher_level_codes(psgc).dropna(how=\"all\")\n",
    "psgc_unpivot.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_loc_names = psgc_unpivot[['code', 'location', 'original']].rename(columns={'code': 'join_code'})\n",
    "all_loc_names.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Region names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psgc_unpivot = psgc_unpivot.merge(all_loc_names.rename(columns={'location': 'adm1_name', 'original': 'adm1_is_orig'}),\n",
    "                                  how=\"left\", left_on=\"adm1_code\", right_on=\"join_code\").drop('join_code', axis=1)\n",
    "psgc_unpivot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Prov names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psgc_unpivot = psgc_unpivot.merge(all_loc_names.rename(columns={'location': 'adm2_name', 'original': 'adm2_is_orig'}),\n",
    "                                  how=\"left\", left_on=\"adm2_code\", right_on=\"join_code\").drop('join_code', axis=1)\n",
    "psgc_unpivot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add MuniCity names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psgc_unpivot = psgc_unpivot.merge(all_loc_names.rename(columns={'location': 'adm3_name', 'original': 'adm3_is_orig'}),\n",
    "                                  how=\"left\", left_on=\"adm3_code\", right_on=\"join_code\").drop('join_code', axis=1)\n",
    "psgc_unpivot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Barangay names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psgc_unpivot = psgc_unpivot.merge(all_loc_names.rename(columns={'location': 'adm4_name', 'original': 'adm4_is_orig'}),\n",
    "                                  how=\"left\", left_on=\"adm4_code\", right_on=\"join_code\").drop('join_code', axis=1)\n",
    "psgc_unpivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special handling for isabela city! it's supposed to be in the province of basilan\n",
    "\n",
    "psgc_unpivot.loc[psgc_unpivot.code.isin([\"099700000\", \"099701000\"]), \"adm2_name\"] = \"BASILAN\"\n",
    "\n",
    "# Also, isabela is the only place with two PSGC codes -- one for province level and one for city level! lets just use one.\n",
    "\n",
    "psgc_unpivot = psgc_unpivot[psgc_unpivot.code != \"099700000\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a \"location tuple\" that concatenates all the location components names into a single tuple. We'll use this for fuzzy matching later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append all the rows again for places in metro manila except with blank regions. \n",
    "#this enables us to accept \"Fort Bonifacio, Taguig\" as an exact match even if it doesn't have \"Metro Manila\" in it\n",
    "\n",
    "metro_manila = psgc_unpivot[psgc_unpivot.code.str.startswith(\"13\")]\n",
    "metro_manila.loc[:, \"adm1_name\"] = np.nan\n",
    "print(len(psgc_unpivot))\n",
    "psgc_unpivot = psgc_unpivot.append(metro_manila, ignore_index=True).reset_index(drop=True)\n",
    "print(len(psgc_unpivot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(item):\n",
    "    replacements = {r\"city of|city\": \"\",\n",
    "                    r\"barangay|brgy\": \"bgy\",\n",
    "                    r\"[^a-zA-Z0-9_\\s]\": \"\",\n",
    "                    r\"poblacion\": \"pob\",\n",
    "                    r\"ñ\": \"n\"}\n",
    "    \n",
    "    item = item.lower()\n",
    "    \n",
    "    if item not in ['bgy', 'municity', 'prov', 'reg']:\n",
    "        \n",
    "        for k, v in replacements.items():\n",
    "            item = re.sub(k,v,item.strip())\n",
    "            \n",
    "    return item.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loc_tuple_with_code(row):\n",
    "    \n",
    "    # For now, disregard making location tuples for all regions \n",
    "    if row.interlevel == \"reg\": \n",
    "        return None \n",
    "    \n",
    "    # Special handling for NCR:\n",
    "    \n",
    "    if row.code[:3] == \"133\" and row.interlevel != \"municity\": # Manila has submunicipalities so include all admin levels\n",
    "        \n",
    "        return tuple([normalize_text(v) for v in [row.adm4_name, row.adm3_name, row.adm2_name, row.adm1_name, row.interlevel, row.code] if (v is not None) and (v is not np.nan)])\n",
    "    \n",
    "    elif (row.code[:3] == \"133\" and row.interlevel == \"municity\") or (row.code[:3] == \"137\"): # Except when Manila City or anywhere else in NCR is the item, then exclude adm2_name\n",
    "        \n",
    "        return tuple([normalize_text(v) for v in [row.adm4_name, row.adm3_name, row.adm1_name, row.interlevel, row.code] if (v is not None) and (v is not np.nan)])\n",
    "        \n",
    "    # else, exclude region from final tuple   \n",
    "    \n",
    "    return tuple([normalize_text(v) for v in [row.adm4_name, row.adm3_name, row.adm2_name, row.interlevel, row.code] if (v is not None) and (v is not np.nan)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psgc_unpivot.loc[:, 'loc_tuple'] = psgc_unpivot.apply(create_loc_tuple_with_code,axis=1)\n",
    "psgc_unpivot.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psgc_unpivot = psgc_unpivot.drop_duplicates(keep=\"first\")\n",
    "len(psgc_unpivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create canonical names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psgc_unpivot.code.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the original names for each unique PSGC code\n",
    "\n",
    "psgc_unpivot['is_canonical'] = psgc_unpivot[['adm4_is_orig', 'adm3_is_orig', 'adm2_is_orig', 'adm1_is_orig']].sum(axis=1)\n",
    "\n",
    "#does every code have a canonical name?\n",
    "\n",
    "psgc_unpivot[psgc_unpivot.is_canonical > 0].code.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the canonical names for each PSGC\n",
    "\n",
    "canonical_names = psgc_unpivot.sort_values([\"code\", \"is_canonical\"], ascending=False).drop_duplicates(\"code\", keep=\"first\")\n",
    "\n",
    "# check if each code has one canonical name\n",
    "\n",
    "canonical_names.groupby('code').size().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonical_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop row for metro manila where region is blank, we don't want to use these in the canonical names\n",
    "\n",
    "null_ncr_region = canonical_names[canonical_names.code.str.startswith(\"13\") * canonical_names.adm1_name.isnull()].index\n",
    "canonical_names = canonical_names.drop(null_ncr_region,axis=0).set_index('code')\n",
    "\n",
    "# rename columns\n",
    "canonical_names = canonical_names.rename(columns={\"adm4_name\": \"bgy\", \"adm3_name\": \"municity\", \"adm2_name\": \"prov\", \"adm1_name\": \"reg\"})\n",
    "\n",
    "# keep only those we need\n",
    "canonical_names = canonical_names[['bgy', 'municity', 'prov', 'reg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(canonical_names.info())\n",
    "canonical_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the canonical names to all possible search terms\n",
    "\n",
    "psgc_locations = psgc_unpivot[['loc_tuple', 'code']].dropna().drop_duplicates(keep=\"first\")\n",
    "psgc_locations = psgc_locations.merge(canonical_names, left_on=\"code\", right_index=True, how=\"left\")\n",
    "psgc_locations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_string(row):\n",
    "    return \",\".join(row.loc_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psgc_locations['loc_tuple'] = psgc_locations.apply(to_string,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psgc_locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psgc_locations['candidate_terms'] = psgc_locations['loc_tuple'].str.rsplit(',', n=1).str.get(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psgc_locations = psgc_locations.set_index('loc_tuple')\n",
    "psgc_locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop region column\n",
    "\n",
    "psgc_locations = psgc_locations.drop('reg',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psgc_locations.to_csv('psgc-locations.csv.gz',compression=\"gzip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "264px",
    "width": "246px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
